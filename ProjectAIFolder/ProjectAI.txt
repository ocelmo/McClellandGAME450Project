#!/usr/bin/env python3
import os
import glob
import random
from pathlib import Path
import sys
from typing import List, Dict, Any

sys.path.append(str(Path(__file__).parents[1]))

import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
import ollama
from util.llm_utils import run_console_chat, tool_tracker, TemplateChat, pretty_stringify_chat, ollama_seed as seed


# Custom embedding function for Ollama
class OllamaEmbeddingFunction:
    def __init__(self, model_name="nomic-embed-text"):
        self.model_name = model_name

    def __call__(self, input: List[str]) -> List[List[float]]:
        response = ollama.embed(model=self.model_name, input=input)
        return response['embeddings']


# Load documents from a directory
def load_documents(data_dir: str) -> Dict[str, str]:
    documents = {}
    for file_path in glob.glob(os.path.join(data_dir, "*.txt")):
        with open(file_path, 'r') as file:
            documents[os.path.basename(file_path)] = file.read()
    return documents


# Chunk documents for embedding
def chunk_documents(documents: Dict[str, str], chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict[str, Any]]:
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
    chunks = []
    for doc_name, content in documents.items():
        for i, chunk in enumerate(text_splitter.split_text(content)):
            chunks.append({"id": f"{doc_name}_chunk_{i}", "text": chunk, "metadata": {"source": doc_name, "chunk": i}})
    return chunks


# Setup ChromaDB with embeddings
def setup_chroma_db(chunks: List[Dict[str, Any]], collection_name: str = "dnd_knowledge", ollama_model: str = "nomic-embed-text") -> chromadb.Collection:
    client = chromadb.Client()
    try:
        client.delete_collection(collection_name)
    except:
        pass
    collection = client.create_collection(name=collection_name, embedding_function=OllamaEmbeddingFunction(ollama_model))
    collection.add(
        ids=[chunk["id"] for chunk in chunks],
        documents=[chunk["text"] for chunk in chunks],
        metadatas=[chunk["metadata"] for chunk in chunks]
    )
    return collection


# Retrieve context for a query
def retrieve_context(collection: chromadb.Collection, query: str, n_results: int = 3) -> List[str]:
    embedding_function = OllamaEmbeddingFunction()
    query_embedding = embedding_function([query])[0]
    results = collection.query(query_embeddings=[query_embedding], n_results=n_results)
    return [doc for doc_list in results.get("documents", []) for doc in doc_list]


# Generate LLM response using context
def generate_response(query: str, contexts: List[str], model: str = "llama3.2") -> str:
    context_text = "\n\n".join(contexts)
    prompt = f"Context: {context_text}\nQuestion: {query}\nAnswer:"
    response = ollama.generate(model=model, prompt=prompt)
    return response["response"]


# Roll dice for DND gameplay
@tool_tracker
def roll_for(skill, dc, player):
    roll = random.randint(1, 20)
    return f"{player} rolled {roll} for {skill} and {'succeeded!' if roll >= int(dc) else 'failed!'}"


# Interactive trade simulation
def run_trader_chat(template_file: str):
    lab04_params = {"template_file": template_file, "sign": "Trader", "end_regex": r"ORDER(.*)DONE"}
    run_console_chat(**lab04_params)


# Interactive chatbot loop
def interactive_dnd_chat():
    sign_your_name = "DND Assistant"
    model = "llama3.2"
    options = {'temperature': 2, 'max_tokens': 50, 'frequency_penalty': 1.5, 'presence_penalty': -1, 'seed': ollama_seed(sign_your_name)}
    messages = [{'role': 'system', 'content': 'Be creative and witty, like a human gamer passionate about DND.'}]
    while True:
        response = ollama.chat(model=model, messages=messages, stream=False, options=options)
        messages.append({'role': 'assistant', 'content': response.message.content})
        print(f"Agent: {response.message.content}")
        user_input = input("You: ")
        if user_input == "/exit":
            break
        messages.append({'role': 'user', 'content': user_input})
    with open(Path("chat_logs.txt"), 'a') as f:
        f.write(pretty_stringify_chat(messages))


# Main function to tie everything together
def main():
    data_dir = "path_to_your_documents"
    documents = load_documents(data_dir)
    chunks = chunk_documents(documents)
    collection = setup_chroma_db(chunks)

    print("Welcome to the DND AI Chat!")
    while True:
        print("1. Ask DND-related questions\n2. Roll dice\n3. Trade simulation\n4. Chat with the assistant\n5. Exit")
        choice = input("Choose an option: ")
        if choice == "1":
            query = input("Ask your question: ")
            contexts = retrieve_context(collection, query)
            response = generate_response(query, contexts)
            print("Response:", response)
        elif choice == "2":
            skill = input("Skill: ")
            dc = input("Difficulty Check: ")
            player = input("Player Name: ")
            print(roll_for(skill, dc, player))
        elif choice == "3":
            run_trader_chat("lab04_trader_chat.json")
        elif choice == "4":
            interactive_dnd_chat()
        elif choice == "5":
            print("Goodbye!")
            break
        else:
            print("Invalid choice. Please try again.")


if __name__ == "__main__":
    main()
