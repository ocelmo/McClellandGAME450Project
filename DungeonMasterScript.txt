import sys
import random
import os
import glob
import json
from pathlib import Path
from typing import List, Dict, Any
import chromadb
from langchain.text_splitter import RecursiveCharacterTextSplitter
import ollama
from util.llm_utils import pretty_stringify_chat, ollama_seed as seed, TemplateChat, tool_tracker

# Dice Roll and Tool Tracker Functions
@tool_tracker
def process_function_call(function_call):
    name = function_call.name
    args = function_call.arguments

    return globals()[name](**args)

def roll_for(skill, dc, player):
    n_dice = 1
    sides = 20
    roll = sum([random.randint(1, sides) for _ in range(n_dice)])
    if roll >= int(dc):
        return f"{player} rolled {roll} for {skill} and succeeded!"
    else:
        return f"{player} rolled {roll} for {skill} and failed!"

def process_response(self, response):
    if response.message.tool_calls:
        self.messages.append({
            'role': 'tool',
            'name': response.message.tool_calls[0].function.name,
            'arguments': response.message.tool_calls[0].function.arguments,
            'content': process_function_call(response.message.tool_calls[0].function)
        })
        response = self.completion()
    return response

# Ollama Embedding Function for RAG
class OllamaEmbeddingFunction:
    def __init__(self, model_name="nomic-embed-text"):
        self.model_name = model_name

    def __call__(self, input: List[str]) -> List[List[float]]:
        response = ollama.embed(model=self.model_name, input=input)
        return response['embeddings']

# RAG Functions
def load_documents(data_dir: str) -> Dict[str, str]:
    documents = {}
    for file_path in glob.glob(os.path.join(data_dir, "*.txt")):
        with open(file_path, 'r') as file:
            content = file.read()
            documents[os.path.basename(file_path)] = content
    print(f"Loaded {len(documents)} documents from {data_dir}")
    return documents

def chunk_documents(documents: Dict[str, str], chunk_size: int = 500, chunk_overlap: int = 50) -> List[Dict[str, Any]]:
    chunked_documents = []
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap, length_function=len)
    for doc_name, content in documents.items():
        chunks = text_splitter.split_text(content)
        for i, chunk in enumerate(chunks):
            chunked_documents.append({"id": f"{doc_name}_chunk_{i}", "text": chunk, "metadata": {"source": doc_name, "chunk": i}})
    print(f"Created {len(chunked_documents)} chunks from {len(documents)} documents")
    return chunked_documents

def setup_chroma_db(chunks: List[Dict[str, Any]], collection_name: str = "dnd_knowledge", ollama_model: str = "nomic-embed-text") -> chromadb.Collection:
    client = chromadb.Client()
    embedding_function = OllamaEmbeddingFunction(model_name=ollama_model)
    try:
        client.delete_collection(collection_name)
    except:
        pass
    collection = client.create_collection(name=collection_name, embedding_function=embedding_function)
    collection.add(
        ids=[chunk["id"] for chunk in chunks],
        documents=[chunk["text"] for chunk in chunks],
        metadatas=[chunk["metadata"] for chunk in chunks]
    )
    print(f"Added {len(chunks)} chunks to ChromaDB collection '{collection_name}'")
    return collection

def retrieve_context(collection: chromadb.Collection, query: str, n_results: int = 3) -> List[str]:
    embedding_function = OllamaEmbeddingFunction()
    query_embedding = embedding_function([query])[0]
    results = collection.query(query_embeddings=[query_embedding], n_results=n_results)
    if "documents" in results and results["documents"]:
        return [doc for doc_list in results["documents"] for doc in doc_list]
    return []

def generate_response(query: str, contexts: List[str], model: str = "llama3.2:latest") -> str:
    context_text = "\n\n".join(contexts)
    prompt = f"""You are a helpful assistant for Dungeons & Dragons players.
    Use the following information to answer the question.
    
    Context:
    {context_text}
    
    Question: {query}
    
    Answer:"""
    response = ollama.generate(model=model, prompt=prompt)
    return response["response"]

# Dungeon Master Class
class DungeonMaster:
    def __init__(self, model_name, trader_file, user_name, data_dir):
        self.model = model_name
        self.trader_file = trader_file
        self.user_name = user_name
        self.data_dir = data_dir
        self.messages = [{'role': 'system', 'content': 'You will act as a DnD Dungeon Master. Take the user on an interesting journey.'}]
        self.chat_options = {'temperature': 2, 'max_tokens': 50, 'frequency_penalty': 1.5, 'presence_penalty': -1}
        self.chat_options |= {'seed': ollama_seed(user_name)}
        self.inventory = self.load_trader_inventory()
        self.collection = None

    def load_trader_inventory(self):
        with open(self.trader_file, 'r') as f:
            data = json.load(f)
            return data.get('inventory', [])

    def setup_rag(self):
        documents = load_documents(self.data_dir)
        chunks = chunk_documents(documents)
        self.collection = setup_chroma_db(chunks)

    def roll_dice(self, sides=6):
        return random.randint(1, sides)

    def roll_for_skill_check(self, skill, dc, player):
        return roll_for(skill, dc, player)

    def retrieve_context_and_respond(self, query):
        if not self.collection:
            print("RAG is not set up. Please run setup_rag first.")
            return "RAG is not available right now."
        contexts = retrieve_context(self.collection, query)
        return generate_response(query, contexts, model=self.model)

    def play_dungeon_game(self, user_input):
        # Example RAG integration
        query_response = self.retrieve_context_and_respond(user_input)
        dice_roll_result = self.roll_for_skill_check("Perception", 15, self.user_name)
        return f"Query Response: {query_response}\nDice Roll Result: {dice_roll_result}"

# Main Execution
if __name__ == "__main__":
    dm = DungeonMaster(
        model_name="llama3.2:latest",
        trader_file="lab04/lab04_trader_chat.json",
        user_name="Tori McClelland",
        data_dir="lab08/data"
    )
    dm.setup_rag()  # Set up RAG before starting the game
    while True:
        user_input = input("Enter your command (/exit to quit): ")
        if user_input == '/exit':
            print("Exiting game. Goodbye!")
            break
        print(dm.play_dungeon_game(user_input))
